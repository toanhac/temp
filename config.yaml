seed_everything: 7
trainer:
  checkpoint_callback: true
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        monitor: val_ExpRate
        mode: max
        filename: '{epoch}-{step}-{val_ExpRate:.4f}'
  gpus: 1
  #gpus: 0,1,2,3
  #accelerator: ddp
  check_val_every_n_epoch: 15
  max_epochs: 300
  deterministic: true

model:
  # Encoder with Feature Fusion (16xâ†’8x)
  d_model: 256
  growth_rate: 24
  num_layers: 16
  fusion_out_channels: 128  # Bottleneck for fusion
  
  # Decoder
  nhead: 8
  num_decoder_layers: 3
  dim_feedforward: 1024
  dropout: 0.4
  
  # Coverage (ARM)
  dc: 32
  cross_coverage: true
  self_coverage: true
  
  # Multi-task learning (auxiliary tasks)
  use_spatial_aux: true
  use_relation_aux: true
  spatial_hidden_channels: 128   # Reduced bottleneck
  relation_hidden_channels: 128
  spatial_loss_weight: 0.1      
  relation_loss_weight: 0.1
  
  # Guided coverage attention
  use_guided_coverage: true
  alpha_spatial: 0.05
  alpha_relation: 0.05
  
  # Beam search
  beam_size: 10
  max_len: 200
  alpha: 1.0
  early_stopping: false
  temperature: 1.0
  
  # Training
  learning_rate: 0.005
  patience: 20

data:
  zipfile_path: data.zip
  test_year: 2014
  train_batch_size: 8
  eval_batch_size: 4
  num_workers: 8
  scale_aug: true
  
  # Multi-task ground truth
  use_spatial_maps: true
  use_relation_maps: true
  gt_cache_dir: data/cached_maps
  generate_spatial_on_fly: false
